#!/usr/bin/env python3

from tqdm import tqdm # Progress bar
import pandas as pd # CSV operations
import numpy as np
import os # File system operations
import argparse # CLI

parser = argparse.ArgumentParser(description='Check detailed vault/archive status')

# filelist should be a text file containing files to check. Such as one generated by:
# find . -type f > filelist.txt
parser.add_argument('-f', '--filelist', help='a text file containing files to check')
parser.add_argument('-p', '--progress_file', help='progress CSV file to read/write to', default="~/recall_progress.csv")
parser.add_argument('-s', '--stats_only', action='store_true', help="just print previously recorded info, don't check files")
parser.add_argument('-v', '--verbose', action='store_true', help='print status for each file to console')

args = parser.parse_args()

def human_readable_size(size, decimal_places=2):
    for unit in ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB']:
        if size < 1024.0 or unit == 'PiB':
            break
        size /= 1024.0
    return f"{size:.{decimal_places}f} {unit}"

def print_summary(df, timestamp):
    df = df[df.run_ended == timestamp]
    n_files_fast = sum(df.current_size_bytes >= df.actual_size_bytes)
    total_files = len(df)
    current_size = df.current_size_bytes.sum() / 2
    actual_size = df.actual_size_bytes.sum()

    timestamp = timestamp.astype('datetime64[s]')
    last_week = timestamp - np.timedelta64(7, 'D')
    n_files_last_week = sum(df.atime >= last_week)
    size_files_last_week = df.current_size_bytes[df.atime >= last_week].sum() / 2

    print(f"""Run ended time: {timestamp}
{n_files_fast}/{total_files} ({n_files_fast / total_files:.2%}) files on fast tier
Size on fast tier: {human_readable_size(current_size)}/{human_readable_size(actual_size)} ({current_size / actual_size:.2%})
Note some tools (like du) will report twice the current file size ({human_readable_size(current_size * 2)}), due to replication between OGG and Tamaki
{n_files_last_week}/{total_files} ({n_files_last_week/total_files:.2%}) files accessed in the week prior (since {last_week})
{human_readable_size(size_files_last_week)}/{human_readable_size(actual_size)} ({size_files_last_week/actual_size:.2%}) accessed in the week prior (since {last_week})
""")

try:
    # Read prior recall status results, if they exist
    df = pd.read_csv(args.progress_file)
    df.timestamp = pd.to_datetime(df.timestamp)
    df.run_ended = pd.to_datetime(df.run_ended)
    df.atime = pd.to_datetime(df.atime)
except:
    df = pd.DataFrame()

if not args.stats_only:

    if args.filelist:
        files = list(pd.read_table(args.filelist, header=None).iloc[:, 0])
    else:
        # If the user doesn't specify which files to check recall status for, just go get updated status for the existing files known in progress_file
        files = df.filepath.unique()

    print(f"Checking {len(files)} files")

    rows = []
    for f in tqdm(files):
        stat = os.stat(f)
        current_size_bytes = stat.st_blocks * 512
        actual_size_bytes = stat.st_size
        if args.verbose:
            try:
                pct = round(current_size_bytes/actual_size_bytes*100, 2)
            except ZeroDivisionError:
                pct = 100
            print(f"{f}: {current_size_bytes/1000/1000}MB / {actual_size_bytes/1000/1000}MB ({pct}%)")
        rows.append({
            "filepath": f,
            "timestamp": pd.Timestamp.now(),
            "current_size_bytes": current_size_bytes,
            "actual_size_bytes": actual_size_bytes,
            "atime": pd.to_datetime(stat.st_atime, unit='s'),
            "mtime": pd.to_datetime(stat.st_mtime, unit='s'),
            "ctime": pd.to_datetime(stat.st_ctime, unit='s')
        })
    new_data = pd.DataFrame(rows)
    new_data["run_ended"] = pd.Timestamp.now()
    df = pd.concat(df, new_data)
    df.to_csv(args.progress_file, index=False)

for timestamp in df.run_ended.unique():
    print_summary(df, timestamp)