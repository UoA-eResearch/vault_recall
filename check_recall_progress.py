#!/usr/bin/env python3

from tqdm import tqdm # Progress bar
from tqdm.contrib.concurrent import thread_map # Parallel operations
import pandas as pd # CSV operations
import numpy as np
import os # File system operations
import argparse # CLI

parser = argparse.ArgumentParser(description='Check detailed vault/archive status')

# filelist should be a text file containing files to check. Such as one generated by:
# find . -type f > filelist.txt
parser.add_argument('-f', '--filelist', help='a text file containing files to check')
parser.add_argument('-p', '--progress_file', help='progress CSV file to read/write to', default="~/recall_progress.csv")
parser.add_argument('-s', '--stats_only', action='store_true', help="just print previously recorded info, don't check files")
parser.add_argument('-v', '--verbose', action='store_true', help='print status for each file to console')

args = parser.parse_args()

def human_readable_size(size, decimal_places=2):
    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:
        if size < 1000 or unit == 'PB':
            break
        size /= 1000
    return f"{size:.{decimal_places}f} {unit}"

def print_summary(df, timestamp):
    df = df[df.run_ended == timestamp]
    on_fast_tier = df.current_size_bytes >= df.actual_size_bytes
    n_files_fast = sum(on_fast_tier)
    total_files = len(df)
    current_size = df.current_size_bytes.sum() / 2
    actual_size = df.actual_size_bytes.sum()

    is_small = df.actual_size_bytes < 1e6
    n_files_small = sum(is_small)
    size_of_small_files = df.actual_size_bytes[is_small].sum()

    timestamp = timestamp.astype('datetime64[s]')
    # The unifiles TSM script only archives files that haven't been touched for 21 days
    N_DAYS = 21
    cutoff_date = timestamp - np.timedelta64(N_DAYS, 'D')
    touched_since_cutoff = df.atime >= cutoff_date
    n_files_touched_since_cutoff = sum(touched_since_cutoff)
    size_files_touched_since_cutoff = df.actual_size_bytes[touched_since_cutoff].sum()

    problem_files = on_fast_tier & ~is_small & ~touched_since_cutoff
    n_problem_files = sum(problem_files)
    size_problem_files = df.actual_size_bytes[problem_files].sum()

    print(f"""Run ended time: {timestamp}
{n_files_fast}/{total_files} ({n_files_fast / total_files:.2%}) files on fast tier
Size on fast tier: {human_readable_size(current_size)}/{human_readable_size(actual_size)} ({current_size / actual_size:.2%})
Note some tools (like du) will report twice the current file size ({human_readable_size(current_size * 2)}), due to replication between OGG and Tamaki
{n_files_small}/{total_files} ({n_files_small / total_files:.2%}) files are less than 1MB
These small files sum to {human_readable_size(size_of_small_files)}
{n_files_touched_since_cutoff}/{total_files} ({n_files_touched_since_cutoff/total_files:.2%}) files accessed in the {N_DAYS} days prior (since {cutoff_date})
{human_readable_size(size_files_touched_since_cutoff)}/{human_readable_size(actual_size)} ({size_files_touched_since_cutoff/actual_size:.2%}) accessed in the {N_DAYS} days prior (since {cutoff_date})
Latest atime: {df.atime.max().floor("S")}
{n_problem_files}/{total_files} ({n_problem_files / total_files:.2%}) unmigrated files that should've been (not touched for 21 days and >=1MB)
Size of unmigrated files: {human_readable_size(size_problem_files)}/{human_readable_size(actual_size)} ({size_problem_files/actual_size:.2%})
""")

def stat_file(f):
    try:
        stat = os.stat(f)
    except:
        print(f"Warning: unable to stat {f}")
        return
    current_size_bytes = stat.st_blocks * 512
    actual_size_bytes = stat.st_size
    if args.verbose:
        try:
            pct = round(current_size_bytes/actual_size_bytes*100, 2)
        except ZeroDivisionError:
            pct = 100
        print(f"{f}: {current_size_bytes/1000/1000}MB / {actual_size_bytes/1000/1000}MB ({pct}%)")
    return {
        "filepath": f,
        "timestamp": pd.Timestamp.now(),
        "current_size_bytes": current_size_bytes,
        "actual_size_bytes": actual_size_bytes,
        "atime": pd.to_datetime(stat.st_atime, unit='s'),
        "mtime": pd.to_datetime(stat.st_mtime, unit='s'),
        "ctime": pd.to_datetime(stat.st_ctime, unit='s')
    }

try:
    # Read prior recall status results, if they exist
    df = pd.read_csv(args.progress_file)
    df.timestamp = pd.to_datetime(df.timestamp)
    df.run_ended = pd.to_datetime(df.run_ended)
    df.atime = pd.to_datetime(df.atime)
except:
    df = pd.DataFrame()

if not args.stats_only:

    if args.filelist:
        files = list(pd.read_table(args.filelist, header=None).iloc[:, 0])
    else:
        # If the user doesn't specify which files to check recall status for, just go get updated status for the existing files known in progress_file
        files = df.filepath.unique()

    print(f"Checking {len(files)} files")

    # Running thousands of stat operations can take a long time, so lets do them in threads
    rows = thread_map(stat_file, files, max_workers=32)
    new_data = pd.DataFrame(rows)
    new_data["run_ended"] = pd.Timestamp.now()
    df = pd.concat([df, new_data])
    df.to_csv(args.progress_file, index=False)

for timestamp in df.run_ended.unique():
    print_summary(df, timestamp)